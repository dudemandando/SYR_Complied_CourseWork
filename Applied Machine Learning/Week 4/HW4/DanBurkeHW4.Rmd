---
title: "HW 4 Federalist Papers"
author: "Dan Burke"
date: "11/2/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation
```{r Preparation}
#load needed libraries
library(stats)
library(dplyr)
library(ggplot2)
library(ggfortify)

#Data Preparation

fedpapers <- read.csv("C:\\Users\\danbu\\Desktop\\Applied Machine Learning\\Week 4\\HW4\\fedPapers85.csv")

#check for Complete Cases
sum(!complete.cases(fedpapers))


#Find Unique Values for Authors
authors <- unique(fedpapers$author)
authors

#drop the file name column
fedpapers <- fedpapers[, !names(fedpapers) %in% c("filename")]

knownpapers <- fedpapers[fedpapers$author == "Hamilton" | fedpapers$author == "Madison" | fedpapers$author == "Jay",]
unknownpapers <- fedpapers[which(fedpapers$author == "HM" | fedpapers$author == "dispt"),]

```
# K Means
```{r KMeans}
set.seed(1234)

#Unsupervised Learning - Convert data to unlabeled
kmeansInput <- data.frame(fedpapers[,-1])

kmeansInput <- scale(kmeansInput, center = T, scale = T)

#WSS Plot to Choose Maximum number of Clusters

wssplot <- function(data, nc=15, seed=1234){
                  wss <- (nrow(data)-1)*sum(apply(data,2,var))
                      for (i in 2:nc){
                set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
              plot(1:nc, wss, type="b", xlab="Number of Clusters",
                            ylab="Within groups sum of squares")
              wss
}

wssplot(kmeansInput)

#K Means Cluster
km = kmeans(kmeansInput, 2, nstart = 30)


#Look at the Cluster Plot
autoplot(km, kmeansInput, frame=TRUE)

#Check Centers
#km$centers

table(fedpapers$author, km$cluster)

```
## Below, I clusterwith only Hamilton, Madison, HM and "dispt" authors. I do this as the disputed papers are disputed between Hamilton and Madison, not to include "Jay".



```{r Kmeans part two}


kmeansInput2 <- data.frame(fedpapers[fedpapers$author == "Hamilton" | fedpapers$author == "Madison" | fedpapers$author == "HM" | fedpapers$author == "dispt" ,])


kmeansInput2scaled <- scale(kmeansInput2[,-1])

#Running K means with 2 clusters
km2 = kmeans(kmeansInput2scaled, 2)

#Plot again with Autoplot
autoplot(km2, kmeansInput2, frame=TRUE)
autoplot(km2, kmeansInput2, colour = 'author',  frame.type = 't') + ggtitle("Seperated by Authors")

#view Cluster Centers
#km2$centers

table(kmeansInput2$author, km2$cluster)

```

## Summary of K Means

After reviewing the results from both outputs (Km & Km2) generated by k- Means. After reviewing "km2, the output points to the conclusion that that all papers possessing the "HM" author attribute are **likely** (based solely on the data provided) to have been authored by James Madison to include 10 of the 11 disputed papers.


# HAC First pass Euclidean Distance 

In the following code block I begin my HAC analysis by preparing the data set without labels. I then normalize that data frame, compute distance and clusters via Euclidean and average methods. This approach gave undesirable results (table provided at end of code block),  which gave no conclusive insight or clues as both Hamilton's and Madison's papers resided within the same cluster. It is because of the contents of cluster "1", that prompted a change in the method of distance calculation.

```{r First Pass Euclidean Dist Method}
set.seed(786)



papers_df <- fedpapers

#Prepare a (Author) paper labels set 
papers_label <- papers_df$author


#Normalize the Data
papers_dfsc <- scale(papers_df[,-1])


#Compute distance via Euclidean method
dist_mat <- dist(papers_dfsc, method = 'euclidean')

#calculate clusters by average method
hclust_avg <- hclust(dist_mat, method = 'average')



plot(hclust_avg)

cut_avg <- cutree(hclust_avg, k = 3)


rect.hclust(hclust_avg , k = 3, border = 2:6)
abline(h = 3, col = 'red')

library(dendextend)

avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 3)
plot(avg_col_dend)

papers_df_cl <- mutate(papers_df, cluster = cut_avg)
count(papers_df_cl,cluster)

table(papers_df_cl$cluster,papers_label)

```
# HAC Second Pass - Manhattan Distance

Utilizing a new distance method, similar results to the first pass were generated. Further exploration was done via different clustering methods also provided inconclusive results. Modifying the k parameter to 4 produced clusters to possessed nearly equal quantities of Hamilton and Madison papers, prompting further exploration in modifying distance and clustering calculation methods.


```{r Manhattan Distance Method}
dist_mat <- dist(papers_dfsc, method = 'manhattan')

hclust_avg <- hclust(dist_mat, method = 'complete')



plot(hclust_avg)

cut_avg <- cutree(hclust_avg, k = 3)


rect.hclust(hclust_avg , k = 3, border = 2:6)
abline(h = 4, col = 'red')

library(dendextend)

avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 4)
plot(avg_col_dend)

papers_df_cl <- mutate(papers_df, cluster = cut_avg)
count(papers_df_cl,cluster)

table(papers_df_cl$cluster,papers_label)

```
# HAC Canberra - First Pass

In an attempt to gain **broad** insight to identifying potential clusters; the following parameters we utilized:
  Distance Method - Canberra
  Clustering Method - Average
  k - 4
  
These parameters produced more insightful, yet not conclusive results. The table displayed (via R code) at the end of the below code block shows a vast majority of Hamilton papers residing within a single cluster (41 total - Cluster 1), along with nearly all Madison papers residing within a seperate cluster (14 - cluster 2). 

This indicates that the distance and cluster methods produced a more favorable result, broadly grouping each of the author's known works within a single cluster. This output does provide hints to who the disputed and "HM" labeled examples can be attributed; however another pass with the same distance and clustering methods, but a reduced K value may provide more meaningful insight (next code block). 


```{r Canberra Distance Method}
dist_mat <- dist(papers_dfsc, method = 'canberra')

hclust_avg <- hclust(dist_mat, method = 'average')



plot(hclust_avg)

cut_avg <- cutree(hclust_avg, k = 4)


rect.hclust(hclust_avg , k = 4, border = 2:6)
abline(h = 4, col = 'red')

library(dendextend)

avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 4)
plot(avg_col_dend)

papers_df_cl <- mutate(papers_df, cluster = cut_avg)
count(papers_df_cl,cluster)

table(papers_df_cl$cluster,papers_label)

```
## HAC Second Attempt - Canberra Distance Summary & Conclution (Estimated Author Attribution)





```{r Canberra Distance Method Reducing Clusters}
dist_mat <- dist(papers_dfsc, method = 'canberra')

hclust_avg <- hclust(dist_mat, method = 'average')



plot(hclust_avg)

cut_avg <- cutree(hclust_avg, k = 3)


rect.hclust(hclust_avg , k = 3, border = 2:6)
abline(h = 4, col = 'red')

library(dendextend)

avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 4)
plot(avg_col_dend)

papers_df_cl <- mutate(papers_df, cluster = cut_avg)
count(papers_df_cl,cluster)

table(papers_df_cl$cluster,papers_label)

```
```{r conclustion}
#HAC Result
print("HAC Result")
table(papers_df_cl$cluster,papers_label)

#K Means Result
print("K Means Result")
table(kmeansInput2$author, km2$cluster)

```
When maintaining the same distance and clustering method parameters, but modifying the k value, we arrive at the most meaningful results thus far (table displayed above "table(papers_df_cl$cluster,papers_label)"). 

Here we see overwhelming clustering of  Hamilton within cluster 1 and Madison within cluster 2. All "HM" (author attribute) example reside within cluster 2 as well as 8 of the disputed papers, given this result and the data provided Madison is most likely to have authored these papers.

# Conclusion

After examining both k-Means and HAC results, their parameters and data provided, it is estimated from this analysis that James Madison is most likely to have written examples labeled both "dispt" and "HM".





